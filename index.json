[{"content":"Workshop Structure Our workshops are tailored to meet the needs of each audience, and we offer a flexible pricing model to accommodate your budget.\nMost workshops are structured with the following three progressive modules:\nLearn (60-90m): This foundation is a lecture-style presentation, followed by a Q\u0026amp;A session. For groups under 25 particpants, there is no additional charge.\nPractice (Half-Day): In this option, we\u0026rsquo;ll go through a hands-on practical component to help participants get a better understanding of the material. Cloud compute environments will be provided, participants need only to bring laptops with internet access.\nPerform (3-Day): In this option, we\u0026rsquo;ll apply what was learned in the practice workshop to your specific application domain.\nPricing Option Base Price Per Participant Presentation + Q\u0026amp;A $3,000 N/A Practice (Half-Day) $2,500 $500 Perform (3-Day) $10,000 $1,500 We understand that travel expenses can be a consideration, so we\u0026rsquo;d be happy to discuss and include these in our final quote for your convenience.\nPlease note that the prices can be adjusted based on the specific needs of your team or organization (e.g., bulk discounts, packaging with other services, etc). If you have any questions or would like to discuss pricing further, please contact us.\n","permalink":"http://mindthemath.com/workshops/info/","summary":"Workshop Structure Our workshops are tailored to meet the needs of each audience, and we offer a flexible pricing model to accommodate your budget.\nMost workshops are structured with the following three progressive modules:\nLearn (60-90m): This foundation is a lecture-style presentation, followed by a Q\u0026amp;A session. For groups under 25 particpants, there is no additional charge.\nPractice (Half-Day): In this option, we\u0026rsquo;ll go through a hands-on practical component to help participants get a better understanding of the material.","title":"General Information About Workshops"},{"content":"Welcome to Mind the Math, where innovation meets mathematics. We are a multidisciplinary consulting firm that specializes in connecting the latest research in academia with the practical application of software development. Our goal is to help teams achieve their outcomes more effectively and efficiently by leveraging the power of technology.\nMichael Pilosov, PhD Mind the Math was founded by Michael Pilosov, a computational mathematician with a unique ability to bring together scientific and creative disciplines in real-world applications. His background in Uncertainty Quantification helped frame many practical engineering concerns (e.g. \u0026ldquo;how do we measure performance\u0026rdquo;, \u0026ldquo;what data is useful?\u0026rdquo;) when leading teams as a Machine Learning Architect to deliver predictive software for major technology companies. He has a deep understanding of both worlds, and he is an expert communicator who can \u0026ldquo;code-switch\u0026rdquo; with ease between the academic and business communities.\nServices We offer a range of services to help teams and organizations accelerate their progress between idea and outcomes. Our services include (but are not limited to):\nWorkshops on leveraging technology to be help you be more effective. Automation: have some manual process that you wish would just run itself? We can help. Training in the latest methods and techniques for data science and machine learning. Consulting on how to apply mathematics and software to achieve outcomes to real-world problems. Technical translation: Let us explain how some new development relates to your specific industry. For more information on our services, please visit our Service Offerings page.\nBridging the Gap Between Theory and Practice Mathematics is the language of all other disciplines, and at Mind the Math, we believe in bridging the gap between theory and practice. By working with us, you benefit from Michael\u0026rsquo;s ability to connect with academics and technologists to understand the latest research and integration with various tooling.\nLet us leverage our expertise in mathematics, software development, and cloud computing to enable rapid iteration between hypotheses and outcomes.\nWhether you\u0026rsquo;re a researcher looking to advance your field, a data science shop seeking to scale workloads and automate processes, or a business looking to harness the power of an exciting new technology, Mind the Math can help.\nLet\u0026rsquo;s explore the possibilities together and innovate at the speed of thought.\n","permalink":"http://mindthemath.com/about/","summary":"Learn about Mind the Math, a multidisciplinary consulting firm that specializes in bridging the gap between theory and applications.","title":"Mind the Math: Innovating at the Speed of Thought"},{"content":"At Mind the Math, we offer a range of services to help teams and organizations achieve their goals more effectively and efficiently. Whether you\u0026rsquo;re looking to improve your data science and machine learning skills, or you need help applying mathematics and software to real-world problems, we have the expertise to help.\nWorkshops Our workshops are designed to educate and train teams in the latest methods and techniques for data science and machine learning. We offer a variety of topics, including:\nThe research-to-production pipeline, from reading papers to reproducing them, as well as sharing your work interactively on the web or with presentations Understanding Git and version control practices for software development Best practices for tracking and retrieving experimental results for collaboration-at-scale Math essentials for measuring the quality of your data engineering pipelines or model performance Strategies for managing and visualizing large datasets and working with images All of our workshops are tailored to your specific audience and needs, and can include presentation, discussion, hands-on components, and a focus on your use case.\nFor more information on the types of workshops we offer, see the Workshops page.\nConsulting We also offer consulting services to help businesses and organizations apply mathematics and software to real-world problems. Our services include:\nProviding guidance on the latest technologies and techniques for data science and machine learning Helping teams to leverage technology to innovate closer to the speed of thought Assisting with the implementation of mathematical models and algorithms in real-world applications Optimizing data pipelines to streamline data processing and improve performance Providing training on software development best practices and workflows Pricing At Mind the Math, we believe in transparent pricing. Our workshops start at $3000, you can find detailed breakdowns on the workshop information page.\nConsulting services are available.\nGet in Touch To learn more about our services and how we can help your team or organization achieve its goals, please contact us. We look forward to connecting with you!\n","permalink":"http://mindthemath.com/services/","summary":"At Mind the Math, we offer a range of services to help teams and organizations achieve their goals more effectively and efficiently. Whether you\u0026rsquo;re looking to improve your data science and machine learning skills, or you need help applying mathematics and software to real-world problems, we have the expertise to help.\nWorkshops Our workshops are designed to educate and train teams in the latest methods and techniques for data science and machine learning.","title":"Service Offerings"},{"content":"Get in Touch Thank you for your interest in Mind the Math. To learn more about our services and how we can help your team or organization, please send us an email at solutions@mindthemath.com.\nSchedule a Call ","permalink":"http://mindthemath.com/contact/","summary":"Get in Touch Thank you for your interest in Mind the Math. To learn more about our services and how we can help your team or organization, please send us an email at solutions@mindthemath.com.\nSchedule a Call ","title":"Contact"},{"content":"Overview In this workshop, we\u0026rsquo;ll be covering how to make your entire paper (figures and all) reproducible. The goal is that anyone who wants to be able to take your raw data and reproduce all of the plots (and underlying statistical analyses being visualized) with the ease of issuing one command / clicking a single button in a web browser.\nReproducibility is a critical aspect to consider when conducting and sharing research. Not only does it strengthen the validity and credibility of findings, but it also facilitates revisiting and building upon work in the future.\nIt\u0026rsquo;s worth noting that reproducibility encompasses not just sharing code, but also ensuring the transparency and trustworthiness of methods and results. Adhering to best practices in reproducibility can enhance the reputation of researchers and bolster trust in their work.\nMotivations Why Care About Reproducibility? Reproducibility is critical for scientific credibility and the validity of research findings. When your work can be easily reproduced by others, it increases the trust and confidence in your results.\nReproducibility is important for your own benefit as well. You never know when you\u0026rsquo;ll want to revisit your work in the future, or when you\u0026rsquo;ll need to build on it. When your work is easily reproducible, it will be much easier for you to pick up where you left off.\nBenefits for Your Future Self Research can be a challenging and time-consuming process, and one frustration researchers often encounter is not being able to easily find or recall the methods and data used to produce their results. By making your work reproducible, you can avoid this frustration and ensure a smooth experience when revisiting your work in the future.\nHaving a clear, concise, and easily accessible record of your methodology and data will save you time and effort. You won\u0026rsquo;t have to waste precious time retracing your steps or trying to figure out how you arrived at your results. This means that you can spend more time building upon your existing work and making new discoveries, rather than being bogged down by the tedious task of reconstructing your research.\nBy adopting best practices in reproducibility, you\u0026rsquo;ll also benefit your future self by fostering a sense of organization and structure in your research. This will make it easier for you to collaborate with others, share your work, and publish your findings, leading to a more impactful and successful research career.\nIn short, making your work reproducible will benefit both your current and future self, allowing you to focus on what truly matters - conducting groundbreaking research.\nApproaches to Reproducibility Even a simple, yet messy, approach to reproducibility is better than having no approach at all. As experience grows, participants may find that a structured approach to code that promotes flexibility and adaptability is even more advantageous, as it can facilitate collaboration and contribution from others, amplifying the impact of the work.\nIn this workshop, various approaches to reproducibility will be covered, from straightforward scripts to more structured and abstracted libraries and packages. The objective is to equip participants with the necessary information to make informed decisions about the best way to reproduce their research and to support them in making their work more accessible and impactful.\nThe workshop will provide participants with the tools and knowledge to make their research reproducible, including LaTeX documents and analysis code, allowing for easy verification and building upon by others.\nPresentation Structure Introduction: In this module, we\u0026rsquo;ll discuss the motivations for making research reproducible, including how your \u0026ldquo;future self\u0026rdquo; will benefit, the importance of scientific credibility, and how reproducibility can save time.\nReproducing the PDF itself (going from images and LaTeX code to a publication-ready paper for submission to arxiv.org), complete with bibliography. Here we\u0026rsquo;ll cover what is involved with LaTeX reproducibility, and talk about tools like latexmake for automating the compilation, with the outcome of providing a docker container capable of taking the source code of the paper in the repo and producing a PDF.\nReproduction of the analyses and figures in the paper. This is about how we provide a reproducible compute environment (enabled by containerization technology, which we\u0026rsquo;ll overview in this section) capable of taking raw data and outputting all of the figures/tables required for inclusion in the final paper.\nLearning Outcomes By the end of this workshop, participants will learn how to:\nUse LaTeX to produce professional-grade research papers, including formatting and bibliography. Automate the process of compiling their paper using tools like latexmake and leverage Makefile for describing dependencies in analyses. Think about how someone else will reproduce all the figures and tables in their paper from raw data. Create reproducible compute environments using containerization technology such as docker and share them (e.g., mybinder.org). Understand the importance of reproducibility in research, and the benefits of making their work easy to reproduce. Workshop Format Presentation: 90 minutes covering the lesson plan above Application: An optional 3-8 hour \u0026ldquo;office hours\u0026rdquo; style hands-on component where participants can practice what they\u0026rsquo;ve learned and receive help with reproducing their own research. This section is limited to members of the same research groups/labs to tailor solutions to specific needs. See the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/reproducible-papers/","summary":"Overview In this workshop, we\u0026rsquo;ll be covering how to make your entire paper (figures and all) reproducible. The goal is that anyone who wants to be able to take your raw data and reproduce all of the plots (and underlying statistical analyses being visualized) with the ease of issuing one command / clicking a single button in a web browser.\nReproducibility is a critical aspect to consider when conducting and sharing research.","title":"Reproducible Research Papers"},{"content":"Overview In this workshop, participants will learn about version control and how it relates to research. We will cover the basics of using Git, a popular version control system, and explore how it can help researchers to manage their work more effectively. We will also discuss different Git repository hosting services, such as Bitbucket, GitHub, and open-source solutions like Gitea, and the benefits and limitations of each.\nObjectives Understand what version control is and why it is important for research Become familiar with Git and the basics of using it for version control Learn how to set up a Git repository and track changes to code and data Explore the different options for Git repository hosting and the benefits and limitations of each Understand how using Git can address some of the pain points in the research process Lesson Outline Introduction to version control\nWhat is version control and why is it important for research The benefits of using version control, including better collaboration, easier tracking of changes, and the ability to go back in time to previous versions Getting started with Git\nOverview of Git and how it works Installing Git on your computer Setting up a Git repository for your research project Tracking changes with Git\nAdding and committing changes to your code and data Understanding Git\u0026rsquo;s branching and merging features Using Git to collaborate with other researchers Hosting your Git repository\nOverview of different Git repository hosting services Setting up a Git repository on a hosting service (e.g. GitHub, Bitbucket, or Gitea) The benefits and limitations of each hosting service Addressing pain points in the research process\nUsing Git to manage changes to your code and data The ability to easily review changes and go back in time to previous versions Using Git to collaborate with other researchers and make your work more accessible to others Learning Outcomes By the end of this lesson plan, participants should have a solid understanding of version control and Git, and be able to effectively use Git to manage their research projects. Whether you are a student, PI, or data scientist in industry, using version control can help you to be more organized, efficient, and collaborative in your research endeavors.\nWorkshop Format See the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/git/","summary":"Overview In this workshop, participants will learn about version control and how it relates to research. We will cover the basics of using Git, a popular version control system, and explore how it can help researchers to manage their work more effectively. We will also discuss different Git repository hosting services, such as Bitbucket, GitHub, and open-source solutions like Gitea, and the benefits and limitations of each.\nObjectives Understand what version control is and why it is important for research Become familiar with Git and the basics of using it for version control Learn how to set up a Git repository and track changes to code and data Explore the different options for Git repository hosting and the benefits and limitations of each Understand how using Git can address some of the pain points in the research process Lesson Outline Introduction to version control","title":"Git Fundamentals: Version-Control for Scientists"},{"content":"Overview This workshop is designed to provide scientists and researchers from academia and industry with a comprehensive understanding of the basics of GNU+Linux. participants will learn how to navigate both local and remote server environments, manipulate file outputs, and install software. Additionally, participants will learn about the basic architecture of Linux/Unix, as well as how to recognize and work with other operating systems found on servers.\nMotivations Comfort with \u0026ldquo;the Terminal\u0026rdquo; is an essential skill for conducting research in a range of scientific fields, and mastering it can help to streamline and speed up the research process. Additionally, becoming proficient in GNU+Linux can open up new opportunities for collaboration and communication with other researchers, many of whom are likely already using Linux/Unix (i.e. MacOS), in their own work.\nThere are many tasks that can be easily automated through the combination of freely available tools (e.g., image-resizing and cropping), awareness is often the limiting factor. Finally, working in the command-line can help participants to develop important technical skills that will be beneficial throughout their careers, and make them more marketable to potential employers.\nLearning Outcomes At the end of this workshop, participants will be able to:\nNavigate the command-line with confidence Manipulate file outputs using sed, grep, and awk Install software and understand the basic architecture of Linux/Unix Recognize and work with other operating systems found on servers Streamline their research and develop important technical skills Lesson Outline Introduction to GNU+Linux and the command-line interface Basic architecture of Linux/Unix, filesystems Where to find pre-installed programs, what to expect on a fresh server Navigating the filesystem with cd, ls Reading, writing, and manipulating files Installing software Other operating systems found on servers Workshop Format This workshop will be delivered through a combination of lectures, hands-on exercises, and group discussions. Please note that this workshop has a minimum time format of 3 hours as it covers a lot of material.\nParticipants will have access to a virtual machine with Linux already installed, so they can practice the skills they learn during the workshop. By the end of the workshop, students will have a solid understanding of the basics of GNU+Linux, as well as the skills and confidence they need to navigate new computational resources for their research.\nSee the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/linux/","summary":"Overview This workshop is designed to provide scientists and researchers from academia and industry with a comprehensive understanding of the basics of GNU+Linux. participants will learn how to navigate both local and remote server environments, manipulate file outputs, and install software. Additionally, participants will learn about the basic architecture of Linux/Unix, as well as how to recognize and work with other operating systems found on servers.\nMotivations Comfort with \u0026ldquo;the Terminal\u0026rdquo; is an essential skill for conducting research in a range of scientific fields, and mastering it can help to streamline and speed up the research process.","title":"Linux Fundamentals: Shells, Servers, and Software"},{"content":"Motivations Images are a fundamental way of representing and communicating information, and digital images have become ubiquitous in modern research. As an academic, you may be interested in analyzing images of all types, from analyzing microscopic images of cells to studying high-resolution images of galaxies. As a data scientist in industry, you may be analyzing video footage and performing image recognition and classification.\nWorking with images is often a complex task, involving specialized software and tools. It can be challenging to manipulate images in a way that accurately reflects the information you are trying to extract. Furthermore, it can be difficult to ensure reproducibility in image analysis workflows.\nThis is where digital image processing with Python comes in. Python provides a powerful, flexible, and easy-to-use language for manipulating digital images. Whether you are working with one image or thousands, Python makes it easy to process, transform, and analyze images in a way that is both efficient and reproducible.\nIn this workshop, we will cover the basics of digital image processing with Python. We will provide an introduction to the math behind digital images, and teach participants how to manipulate them with code. We will cover many common image transformations that are of interest to those analyzing datasets of images, be they of stars or cars.\nLearning Outcomes By the end of this lecture, you will have learned:\nThe basics of digital image processing, including color models and image transforms. How to read, write, and display images using numpy, Pillow and OpenCV. How to transform pixels, resize images, and apply masks How to apply some common image processing techniques to an image, such as smoothing, sharpening, and thresholding. How to use Python to perform simple measurements and calculations on an image. Through a series of examples and exercises, you will gain a practical understanding of how to use Python to manipulate images, and gain confidence in your ability to incorporate image processing techniques into your research. Worshop Format See the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/image-processing/","summary":"Motivations Images are a fundamental way of representing and communicating information, and digital images have become ubiquitous in modern research. As an academic, you may be interested in analyzing images of all types, from analyzing microscopic images of cells to studying high-resolution images of galaxies. As a data scientist in industry, you may be analyzing video footage and performing image recognition and classification.\nWorking with images is often a complex task, involving specialized software and tools.","title":"Digital Image Processing with Python"},{"content":"Motivations In scientific research and data analysis, we often need to quantify and compare differences between data sets. Metrics, such as distances, divergences, and similarity scores, are essential mathematical tools for this task. They allow us to define and measure how different two data sets are, or how well a model fits a given dataset.\nIn machine learning, metrics are commonly used to assess the performance of models, compare the accuracy of different algorithms, or to tune model parameters. But the same concepts and techniques are also useful in many other scientific disciplines, such as ecology, biology, physics, social sciences, and more.\nHowever, choosing the right metric for a specific task can be a challenging task, especially when multiple metrics are available, and it is not always obvious which one is the most appropriate for a given problem. Moreover, different metrics often have different properties and assumptions, and they can yield very different results depending on the data being analyzed.\nThe goal of this workshop is to introduce participants to the most commonly used metrics in scientific research and machine learning, and to provide them with a solid understanding of their mathematical properties, applications, and limitations. By the end of the workshop, participants will be able to select, apply, and interpret different metrics to compare data sets, assess the quality of models, and solve a wide range of data analysis problems.\nLearning Outcomes By the end of this workshop, you will be able to:\nUnderstand the mathematics behind distance calculations and formal metrics Compare and contrast common metrics used across scientific disciplines and machine learning applications Measure differences in distributions Relate different metrics to one another Apply these concepts to real-world data and research problems Workshop Format This workshop will consist of a mix of presentation materials and interactive coding examples that will give participants a chance to practice the concepts that we cover. Here\u0026rsquo;s a rough breakdown of how the workshop will be structured:\nIntroduction (10 minutes) - We\u0026rsquo;ll start by providing an overview of the goals and objectives for the workshop, as well as the schedule for the session. Theory (20 minutes) - Next, we\u0026rsquo;ll cover the theoretical underpinnings of formal metrics and distance calculations, including some of the most commonly used metrics in scientific disciplines and machine learning applications. Metrics Presentation \u0026amp; Coding Examples (60 minutes) - We\u0026rsquo;ll cover a selection of metrics (tailored to the audience) and intersperse several interactive sections. Participants will then have an opportunity to try out some of the concepts covered in the presentation using interactive code examples that they can run in their web browsers. Q\u0026amp;A and Wrap-up (10 minutes) - We\u0026rsquo;ll end the workshop with a brief Q\u0026amp;A session to ensure that all participants\u0026rsquo; questions have been answered and to provide additional resources for further learning. By incorporating live coding examples throughout the workshop, we hope to make the material more engaging and interactive, and give participants the opportunity to apply what they\u0026rsquo;ve learned in a practical way. See the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/metrics/","summary":"Motivations In scientific research and data analysis, we often need to quantify and compare differences between data sets. Metrics, such as distances, divergences, and similarity scores, are essential mathematical tools for this task. They allow us to define and measure how different two data sets are, or how well a model fits a given dataset.\nIn machine learning, metrics are commonly used to assess the performance of models, compare the accuracy of different algorithms, or to tune model parameters.","title":"Metrics Menagerie: Understanding Metrics for Scientific and ML Applications"},{"content":"Overview Presenting research to a scientific or academic audience can be a daunting task, especially if you are not confident in your communication skills. This workshop will provide you with the skills to enhance your presentation abilities and help you effectively communicate your research. We will cover the basics of presentation skills, as well as the use of LaTeX to create professional posters and presentations. Whether you are a student, a postdoctoral researcher, or a principal investigator, this workshop will help you communicate your research in a clear, concise, and visually appealing manner.\nMotivations As researchers, it is crucial to communicate your findings effectively in order to have a meaningful impact on your field and on society. Whether you are presenting your work at a conference, publishing in a journal, or sharing your results with collaborators, it is essential to present your work in a way that is clear, concise, and visually appealing. With the increasing use of technology in scientific communication, it is important to be equipped with the skills to create professional posters and presentations using LaTeX, a powerful typesetting system that is widely used in the academic community.\nLearning Outcomes Understand the basics of presentation skills and how to effectively communicate research results Learn how to structure your presentations and posters for maximum impact Acquire the skills to use LaTeX and Beamer to create professional posters and presentations Explore the use of alternative tools for creating posters and presentations when time is limited or a specific style is desired Lesson Outline Introduction to Presentation Skills The importance of clear and concise communication Tips for making effective presentations How to structure your presentation or poster for maximum impact Introduction to LaTeX Overview of LaTeX and its use in the academic community The basics of using LaTeX for posters and presentations Creating Presentations with Beamer Introduction to Beamer, the LaTeX Package for Creating Presentations Advantages of Beamer over other presentation software Installing Beamer Basic structure of a Beamer presentation LaTeX for Poster Creation: Introduction to TikZ, the LaTeX Package for Creating Posters Advantages of TikZ over other poster creation software Installing TikZ Basic structure of a TikZ poster Formatting your poster in TikZ Themes and templates Including figures, tables, and multimedia Creating a custom theme Best practices for poster creation in TikZ Organizing content for maximum impact Avoiding clutter and maintaining a clear focus Creating an aesthetically pleasing layout Alternatives to LaTeX for Poster Creation When to use software like Canva or Adobe Illustrator When to use LaTeX Comparing and contrasting the strengths and weaknesses of each option Workshop Format The workshop will consist of a 90-minute presentation that covers the basics of effective research communication, including the use of LaTeX for presentations and posters. Following the presentation, there will be two optional modules.\nThe first is a one-hour poster critique session, where participants can apply the concepts covered in the presentation to real-world examples.\nThe second is a 2-4 hour hands-on session, where participants will work with web-based LaTeX software, such as Overleaf and MyBinder, to create their own posters. This session will provide participants with a more granular level of control over their computational environment, allowing them to experiment and refine their skills.\nBy the end of the workshop, participants will have a solid understanding of how to use LaTeX for creating effective and professional-looking presentations and posters for their research.\nSee the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/presentations/","summary":"Overview Presenting research to a scientific or academic audience can be a daunting task, especially if you are not confident in your communication skills. This workshop will provide you with the skills to enhance your presentation abilities and help you effectively communicate your research. We will cover the basics of presentation skills, as well as the use of LaTeX to create professional posters and presentations. Whether you are a student, a postdoctoral researcher, or a principal investigator, this workshop will help you communicate your research in a clear, concise, and visually appealing manner.","title":"Making Effective Posters and Presentations with LaTeX"},{"content":"Introduction JupyterHub is a multi-user version of Jupyter Notebook, an open-source web application for interactive computing and data science. By deploying JupyterHub with Docker Compose, teams and labs of academic researchers can benefit from its features, such as increased computational reproducibility, fast and easy onboarding of new team members, and an operating system agnostic interface.\nNote: This workshop is primarily targeted at PI\u0026rsquo;s and members of faculty/administration, but students are also welcome to attend (and perhaps become their group\u0026rsquo;s \u0026ldquo;maintainer\u0026rdquo;). Students can learn to maintain their own research environments, which will improve their individual contributions, but the greatest increases in productivity will come from adoption at the lab (or research group) level.\nAdvantages of using JupyterHub By deploying JupyterHub with Docker Compose, academic researchers and teams can greatly benefit from its advantages, including increased computational reproducibility, fast onboarding of new members, and an operating system agnostic interface. The ease and flexibility of deploying JupyterHub with Docker Compose make it an ideal tool for collaborative research and data science.\nComputational Reproducibility: Docker containers allow for easy replication and distribution of research environments, ensuring that all team members are using the same software, libraries, and configurations. Onboarding: New team members can quickly begin research by avoiding all the computational set-up. They have a working environment from the first day to begin focusing on innovating, not managing dependencies. OS Agnostic Interface: By using JupyterHub through a web browser, the underlying operating system of team members\u0026rsquo; laptops or machines becomes less important. The laptop becomes a \u0026ldquo;thin-client\u0026rdquo;, and all necessary tools are provided through the JupyterHub environment. Many different compute environments can be accessed through one consistent web-browser interface (to accomodate different lines of research inquiry). Extensibility: By understanding the instrastructure underpinning Jupyter and JupyterHub, you can leverage it\u0026rsquo;s open-source framework to manage and host any web-application as well as backend software to support it. Workshop Overview In this workshop, we will cover the process of deploying JupyterHub using Docker-Compose, including:\nAn overview of JupyterHub and why it\u0026rsquo;s beneficial to teams of all shapes and sizes An introduction to Docker-Compose and why it\u0026rsquo;s well-suited to deploying JupyterHub Steps for setting up and deploying a JupyterHub instance using Docker-Compose Discussion of best practices for managing and scaling JupyterHub deployments Learning Outcomes By the end of this workshop, participants will:\nUnderstand the benefits of using JupyterHub for teams of academic researchers and industry data scientists Have seen the entire process of deploying and configuring JupyterHub using Docker-Compose Be equipped to manage and scale JupyterHub deployments in their own academic environments Workshop Structure This is a longer workshop, with a minimum time format of 3 hours. See the workshop structure page for more information about general workshop format.\nIntroduction to JupyterHub What is JupyterHub Benefits of using JupyterHub for academic research teams Key use cases for JupyterHub in a research environment Using JupyterHub in a Team Setting Sharing and collaborating on notebooks and other dat Setting up a computational environment that is consistent across team members, using different configurations Ensuring computational reproducibility and reproducible results Getting new team members \u0026ldquo;spun up\u0026rdquo; quickly and efficiently Introduction to Docker-Compose What is Docker-Compose and how does it work Benefits of using Docker-Compose for deploying JupyterHub Setting up JupyterHub with Docker-Compose Step-by-step guide to deploying a JupyterHub instance using Docker-Compose Configuring JupyterHub with Docker Compose Setting up the compute environment (what JupyterHub will leverage) Discussion of available options and configurations Advanced Topics in JupyterHub Deployment Customizing the JupyterHub user interface Managing user accounts with LDAP or OAuth vs HashAuth Scaling JupyterHub to accommodate large numbers of users, how to assess your specific needs Troubleshooting and maintenance of JupyterHub Hands-on Practice and Discussion Participants will follow along for the deployment of JupyterHub to which they\u0026rsquo;ll be able to connect and interact Conclusion and Next Steps Recap of key takeaways from the workshop Opportunities for further learning and exploration Q\u0026amp;A session ","permalink":"http://mindthemath.com/workshops/jupyterhub/","summary":"Introduction JupyterHub is a multi-user version of Jupyter Notebook, an open-source web application for interactive computing and data science. By deploying JupyterHub with Docker Compose, teams and labs of academic researchers can benefit from its features, such as increased computational reproducibility, fast and easy onboarding of new team members, and an operating system agnostic interface.\nNote: This workshop is primarily targeted at PI\u0026rsquo;s and members of faculty/administration, but students are also welcome to attend (and perhaps become their group\u0026rsquo;s \u0026ldquo;maintainer\u0026rdquo;).","title":"Deploying JupyterHub with Docker-Compose: A Workshop for Academic Teams"},{"content":"Jupyter Notebooks are a powerful tool for researchers to organize and document their research process. In this workshop, we will cover best practices for using Jupyter Notebooks in research and provide hands-on demonstrations to help attendees better understand the benefits and limitations of this tool.\nLearning Outcomes Understanding the basics of Jupyter Notebooks and how they can be used in research Best practices for organizing and documenting research processes within Jupyter Notebooks Hands-on demonstrations of using Jupyter Notebooks to organize, document, and visualize research results Understanding the limitations of Jupyter Notebooks and how to overcome them Tips and tricks for using Jupyter Notebooks effectively in research Who Should Attend This workshop is designed for academics and industry professionals who are looking to better organize their research process and take advantage of the benefits of Jupyter Notebooks. No prior experience with Jupyter Notebooks is required.\nJupyter Notebooks in the Research Process Jupyter notebooks are a powerful tool for researchers to organize, document, and share their work. In this workshop, we will cover best practices for effective use of Jupyter notebooks as they pertain to interests and goals in the research \u0026ldquo;pipeline\u0026rdquo;.\nBenefits of Using Jupyter Notebooks Reproducibility: Jupyter notebooks allow you to document every step of your research process, from data import to analysis and visualization. This makes it easy to share your work and recreate your results.\nCollaboration: Jupyter notebooks allow multiple people to work on the same project, making it easier to collaborate with others.\nEase of use: Jupyter notebooks provide an intuitive interface that makes it easy to code, experiment, and visualize results.\nBest Practices Organization: Keep your notebooks organized by using clear and descriptive names for each notebook, adding relevant tags and descriptions, and using a consistent file structure.\nCode modularity: Break down your code into smaller, reusable pieces by creating functions and modules. This makes it easier to maintain your code and reuse parts of it for future projects.\nDocumentation: Add comments and markdown cells to document your code and explain what you are doing. This makes it easier for others to understand your work and for you to look back at your code later on.\nVersion control: Use version control (such as Git) to keep track of changes to your notebooks and collaborate with others.\nPackaging: Put re-usable code into a centralized place and use your notebooks to import these functionalities\nTesting: Use unit tests to validate your code and make sure that it is working as expected.\nVisualization: Use visualizations to help explain and communicate your results. Make sure that your visualizations are clear and informative.\nSharing: Share your notebooks with others by using platforms such as GitHub or Google Colab.\nBy following these best practices, you can make the most of Jupyter notebooks in your research process and ensure that your work is organized, understandable, and reproducible.\nWorkshop Structure Introduction to Jupyter Notebooks\nOverview of Jupyter Notebooks How to install and launch Jupyter Notebook Jupyter Notebook interface and basic functionality Best Practices for Jupyter Notebook Usage in Research\nOrganizing notebooks into a structure that makes sense for your project Writing clear and concise markdown cells Storing and version controlling notebooks with Git Using Jupyter Notebooks to record and share findings with collaborators JupyterHub: Collaborative Jupyter Notebooks\nWhat is JupyterHub How to install and launch JupyterHub Best practices for hosting JupyterHub for a team Managing users, authentication and access control Setting up a multi-user environment for reproducible research Hands-On Activities\nSetting up a Jupyter Notebook for a research project Working with Markdown cells and LaTeX equations Using Jupyter Notebook to perform data analysis and visualizations Sharing Jupyter Notebooks with collaborators Setting up and using JupyterHub for a team of researchers Conclusion\nSummary of key concepts learned Discussion of future directions for Jupyter Notebook usage in research Q\u0026amp;A session to address any remaining questions See the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/jupyter/","summary":"Jupyter Notebooks are a powerful tool for researchers to organize and document their research process. In this workshop, we will cover best practices for using Jupyter Notebooks in research and provide hands-on demonstrations to help attendees better understand the benefits and limitations of this tool.\nLearning Outcomes Understanding the basics of Jupyter Notebooks and how they can be used in research Best practices for organizing and documenting research processes within Jupyter Notebooks Hands-on demonstrations of using Jupyter Notebooks to organize, document, and visualize research results Understanding the limitations of Jupyter Notebooks and how to overcome them Tips and tricks for using Jupyter Notebooks effectively in research Who Should Attend This workshop is designed for academics and industry professionals who are looking to better organize their research process and take advantage of the benefits of Jupyter Notebooks.","title":"Effective Use of Jupyter Notebooks in Research Process"},{"content":"Overview In this workshop, participants will learn about MLflow, an open source platform for the complete machine learning life cycle. The focus of the workshop will be on how to use MLflow to organize, track and reproduce research experiments, ensuring the reproducibility of results and enabling collaboration among research teams.\nMotivations As a researcher practitioner, one faces numerous challenges when organizing and tracking experiments. Experimenting on different machines, with varying configurations and dependencies can lead to discrepancies in results and difficulties in reproducing results. Keeping track of multiple experiments and determining the best approach to a problem can also be a cumbersome task.\nMLflow presents a solution to these challenges by providing a centralized platform for tracking experiments, organizing code and reproducing results. This makes research more efficient and trustworthy.\nThe process of organizing and tracking research experiments becomes streamlined and simplified with MLflow. A centralized platform for logging information related to models, such as parameters, code versions and results, is provided by MLflow. MLflow makes it easy to track down previous parameters and figures used in experiments, as all the relevant information is stored in one place. MLflow abstracts away the maintenance of a database, allowing researcher practitioners to focus on logging important information without being burdened by technical details. Similarly it abstracts away the maintenance of artifacts (data, figures), saving them under a common structure associated with each experimental run. MLflow\u0026rsquo;s ability to simplify and streamline the process of organizing and tracking research experiments makes it an indispensable tool for researcher practitioners.\nLearning Outcomes At the end of this workshop, participants will be able to:\nSet up and use MLflow to track experiments Use MLflow to organize code and reproducing results Compare and reproduce results from multiple experiments Collaborate with research teams by sharing experiments and results Lesson Outline Introduction to MLflow and its components Setting up MLflow and tracking experiments Logging code and results in MLflow Reproducing experiments and results Comparing experiments and results in MLflow Collaborating with research teams using MLflow Workshop Format Presetantation: The workshop will consist of a 45-minute presentation that covers the basics of MLflow and its benefits for organizing research experiments. Practice: An optional multi-hour hour workshop where participants will walk through the process of deploying MLflow alongside an artifact store and database by using docker-compose. This hands-on experience will give participants a practical understanding of how to use MLflow in their own research projects. See the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/mlflow/","summary":"Overview In this workshop, participants will learn about MLflow, an open source platform for the complete machine learning life cycle. The focus of the workshop will be on how to use MLflow to organize, track and reproduce research experiments, ensuring the reproducibility of results and enabling collaboration among research teams.\nMotivations As a researcher practitioner, one faces numerous challenges when organizing and tracking experiments. Experimenting on different machines, with varying configurations and dependencies can lead to discrepancies in results and difficulties in reproducing results.","title":"Organizing Research with MLflow"},{"content":"Overview In this workshop, you will learn how to package your Python code and publish it on PyPI, the Python Package Index, for others to use.\nThis workshop is targeted towards Python developers who are looking to distribute their code as a package, and who want to learn how to do it properly. You will learn best practices for packaging Python code, and walk through the process of creating a package from scratch.\nMotivations Packaging your code as a library or module is important for sharing your work with others, as it makes it easy for others to use your code in their own projects. This is especially important in the scientific community, where collaboration and reproducibility are crucial.\nFor Academics: Being comfortable with Python packaging can be incredibly helpful for researchers looking to publish their work in academic journals. By packaging their code into a reusable and sharable format, they can increase the transparency and reproducibility of their work.\nPackaging code makes it easier for other researchers to understand the implementation of novel algorithms and to build on them, leading to increased collaboration and faster scientific progress. Furthermore, it becomes simpler to create and share research software libraries, which can benefit the wider research community.\nBecoming proficient in Python packaging is a valuable skill for any researcher looking to increase the impact and visibility of their work.\nFor Everyone: Creating and distributing a Python package also has other benefits, such as:\nFacilitating code maintenance and reuse Making it easier to manage dependencies and ensure compatibility with other projects Allowing you to version your code and manage releases more effectively Helping to establish your code as a reputable and reliable tool in the community Learning Outcomes By the end of this workshop, you will be able to:\nUnderstand the structure of a Python package Create a setup.py file and a package directory for your code Use setuptools to build and distribute your package Publish your package on PyPI Workshop Format This workshop will be structured as a combination of lectures, demonstrations, and hands-on exercises. You will be provided with a sample project that you will use to package and distribute your code.\nNote: conda packaging would encompass a separate followup workshop, as it is quite a bit more involved.\nSee the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/packaging/","summary":"Overview In this workshop, you will learn how to package your Python code and publish it on PyPI, the Python Package Index, for others to use.\nThis workshop is targeted towards Python developers who are looking to distribute their code as a package, and who want to learn how to do it properly. You will learn best practices for packaging Python code, and walk through the process of creating a package from scratch.","title":"Python Packaging Masterclass"},{"content":"Overview This workshop will provide participants with the tools and techniques to effectively communicate their research results using Matplotlib, a powerful and versatile plotting library in Python. Participants will learn how to use colormaps for colorblind accessibility, how to control label sizes and formatting legends, and how to think about manipulation of the visualizations they are trying to create.\nIn this workshop, we will delve into the importance of writing re-usable functions for plotting. By having a well-structured code base, researchers can enjoy numerous benefits, including:\nEfficiency: By having a centralized source of plotting code, updates and changes can be made more quickly and easily, saving valuable time and allowing researchers to focus on their analysis and interpretation of results.\nReduced Errors: A well-architected code base can help reduce the risk of errors that might arise from duplicating code or making inconsistent changes. By having a single source of truth for plotting code, researchers can ensure that all plots are consistent and accurate.\nFlexibility: Re-usable functions can be designed with flexibility in mind, so that they can be adapted to new data or updated to reflect changes in standards or best practices. This makes it easier to keep the code up-to-date and responsive to evolving needs.\nAccelerating the Research-to-Paper Pipeline In this workshop, we will keep in mind the ultimate goal of industry scientists and academics: publishing their research results. Effective communication of results is critical for advancing their careers, and well-structured plotting code can help them reach this goal.\nBy writing re-usable functions for plotting, participants will be able to quickly and easily make changes to their plots based on feedback from advisors, peers, or other reviewers. This eliminates the need to spend time on manual formatting, freeing up more time for innovation and analysis.\nAdditionally, having a well-architected code base reduces the risk of duplicating code or making inconsistent changes, increasing the likelihood that their results will be accepted for publication. In short, the skills and techniques taught in this workshop will help participants communicate their results more effectively and efficiently, enabling them to focus on what they do best: advancing their research.\nLearning Outcomes Learning Outcomes By the end of this workshop, participants will be able to:\nOrganize and label their results using figure and axes objects for effective communication. Control label sizes, font choices, and overall plot formatting for clarity and readability. Create legends that accurately convey the information in their plots and match the overall style. Utilize colormaps for colorblind accessibility and understand the importance of color in communication. Choose appropriate file formats and resolutions (vector vs pixel representations) for different purposes, such as publication, presentation, or online sharing. Navigate the source code and documentation for matplotlib effectively, and use it to address their specific needs and preferences. Write re-usable functions for plotting that can be easily modified in one place to address updates and changes. Reduce time and effort spent on manual formatting and duplicated code, freeing up more time for innovation and analysis. Communicate their results more effectively and efficiently, increasing the likelihood of their results being accepted for publication. Workshop Format The workshop is organized in the following modules:\nPresentation: A 50-80 minute presentation will provide an overview of the concepts and techniques for effective and organized plotting using matplotlib.\nHands-on component: An optional 1-3 hour \u0026ldquo;office hours\u0026rdquo; style component will give participants the opportunity to practice what they\u0026rsquo;ve learned in a sandbox environment provided by the workshop. This will allow participants to manipulate plots, play with formatting improvements, and see the results of their changes in real-time, using a variety of realistic datasets (geospatial, image, time-series, etc).\nApplication: An optional 1-3 hour \u0026ldquo;office style\u0026rdquo; component will cover applying the lessons of module (2) towards their specific research goals. participants will bring examples of their own data visualization results and we will work on incorporating the lessons learned. This section also presents an opportunity to collaborate and identify common issues they face while plotting their data. By combining their efforts, they can save time on plotting results and focus more on communicating them effectively and rapidly.\nThe hands-on and application components are designed to be supportive and interactive experiences, where participants can ask questions, receive feedback, and work at their own pace. These components provide valuable opportunities for participants to solidify their understanding of the material, apply it to real-world scenarios, and collaborate with their peers.\nSee the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/matplotlib/","summary":"Overview This workshop will provide participants with the tools and techniques to effectively communicate their research results using Matplotlib, a powerful and versatile plotting library in Python. Participants will learn how to use colormaps for colorblind accessibility, how to control label sizes and formatting legends, and how to think about manipulation of the visualizations they are trying to create.\nIn this workshop, we will delve into the importance of writing re-usable functions for plotting.","title":"Visualizing Data with Matplotlib"},{"content":"Overview In this workshop, participants will learn about the process of sharing their research with the broader public through interactive web applications. This workshop will cover the basics of how the web works, what participants need to know about container technology to deploy their app on any cloud or server. We will also cover various options available for sharing research, from full-featured applications built with streamlit, to API endpoints with flask or gunicorn, to simple static web pages hosted for free on cloud providers like GitHub Pages.\nFurthermore, for a hands-on component, we will demonstrate how to build a full-stack web application for conducting A/B testing on experimental results. Using a combination of streamlit for the front-end and mlflow for tracking and logging results, attendees will learn how to build and leverage a web app for answering their own research questions.\nMotivations Academics are often focused on conducting thorough and rigorous research, but may overlook the importance of sharing their findings with a wider audience. Sharing research results through interactive web applications not only makes it easier for the general public to access and understand the research, but can also improve the research process itself.\nFor example, an interactive web application can be used to present research results in an engaging and accessible manner, making the work more impactful and potentially reaching a larger audience. On the other hand, academics can also use these applications to orchestrate experiments and visualize results in a user-friendly interface, making it easier to analyze and interpret the data.\nSharing research results with the broader public is an important aspect of the scientific process. By making research results accessible and interactive, researchers can help people understand complex concepts, engage with the research community, and receive feedback and criticism that can help improve their work.\nLearning Outcomes At the end of this workshop, participants will be able to:\nExplain the basics of how the web works Choose a suitable container technology to deploy their app on any cloud or server Evaluate the various options for sharing research, including full-featured applications, API endpoints, and static web pages Share their research with the broader public using one of the options covered in the workshop Lesson Outline Introduction to the basics of how the web works, what you need to know to share your work Understanding container technology for deploying web applications Overview of the options for sharing research, including streamlit, API endpoints with flask and gunicorn, and static web pages hosted on cloud providers Hands-on demonstration of deploying a research app using one of the options covered in the workshop Workshop Format Presentation: The workshop will consist of a 45-minute presentation that covers the basics of how the web works and the various options for sharing research. Practice: An optional multi-hour workshop where participants will have the opportunity to work with a demonstration of deploying a research app using one of the options covered in the workshop. See the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/web-apps/","summary":"Overview In this workshop, participants will learn about the process of sharing their research with the broader public through interactive web applications. This workshop will cover the basics of how the web works, what participants need to know about container technology to deploy their app on any cloud or server. We will also cover various options available for sharing research, from full-featured applications built with streamlit, to API endpoints with flask or gunicorn, to simple static web pages hosted for free on cloud providers like GitHub Pages.","title":"Sharing Research with the Public: Interactive Web Applications"},{"content":"Motivations When deploying Docker containers, it\u0026rsquo;s important to understand that by default, a container\u0026rsquo;s network interface is only accessible by other containers running on the same host. This can be problematic if you need to expose a container\u0026rsquo;s services to the outside world. To do this, you need to create a network bridge between the host and the container.\nOnce the network bridge is established, you\u0026rsquo;ll then need to consider which ports you need to expose to the outside world. By default, a container\u0026rsquo;s ports are not exposed to the host or external network. You can use the docker run command\u0026rsquo;s -p flag to expose a container\u0026rsquo;s ports to the host machine or to the public internet.\nThere are several best practices to consider when managing container networking and exposing ports to ensure that your containers are secure. It\u0026rsquo;s important to cover these best practices during the workshop to ensure that participants understand how to safely manage container networking in their own environments.\nCommon Pitfalls For the Docker security workshop, there are several common pitfalls that participants will encounter, and we\u0026rsquo;ll make sure to cover how to address them.\nFirst, one common issue is the use of outdated or vulnerable base images, which can lead to security vulnerabilities in the container. We\u0026rsquo;ll discuss how to choose secure base images and keep them updated.\nAnother pitfall is failing to isolate container environments from the host system or other containers on the same network, which can lead to unauthorized access or data breaches. We\u0026rsquo;ll cover how to use network isolation and container permissions to prevent these issues.\nFinally, many users don\u0026rsquo;t properly secure access to container registries or manage access to images, which can lead to malicious users gaining access to sensitive data. We\u0026rsquo;ll cover the use of authentication and authorization measures to properly secure container images and registries.\nAn sampling of topics that may be covered (tailored to audience interest):\nFailure to keep base images updated with security patches Failure to set up correct user permissions within the container Running processes as root within the container Exposing too many ports or exposing ports without proper configuration Improper volume or bind-mount configuration, leading to unwanted data exposure Using untrusted third-party images without verifying their source or contents Storing sensitive information, such as passwords or credentials, in the container Running unneeded services or processes within the container, which can provide additional attack vectors Failure to properly implement access controls and authentication for the container Lack of monitoring and logging within the container, making it difficult to identify and respond to security events It\u0026rsquo;s important to be aware of these potential pitfalls in order to maintain the security of Docker containers and the systems on which they run. In our workshop, we\u0026rsquo;ll discuss these topics in detail and provide guidance on how to avoid these issues.\nWorkshop Format This workshop will cover a range of security considerations for Docker containers, including networking and port forwarding. Participants will learn how to secure their Docker containers and prevent potential security breaches. We will go through a number of practical exercises, where participants will have the opportunity to explore and understand the following topics:\nOverview of Docker networking concepts and how they work Differences between Docker\u0026rsquo;s default bridge network and user-defined networks How to use docker network create to create and manage user-defined networks Exposing ports with Docker, including the -p and \u0026ndash;expose options Docker port forwarding, including the -p and \u0026ndash;publish options Best practices for securing Docker containers, networks, and images The workshop will be structured to include a mix of lecture-style presentation and hands-on exercises to help participants gain practical experience with Docker security concepts.\nSee the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/docker-security/","summary":"Motivations When deploying Docker containers, it\u0026rsquo;s important to understand that by default, a container\u0026rsquo;s network interface is only accessible by other containers running on the same host. This can be problematic if you need to expose a container\u0026rsquo;s services to the outside world. To do this, you need to create a network bridge between the host and the container.\nOnce the network bridge is established, you\u0026rsquo;ll then need to consider which ports you need to expose to the outside world.","title":"Securing Docker Containers: Best Practices and Common Pitfalls"},{"content":"Motivations Research papers are an integral part of academic research. They are the primary means of disseminating research findings, which makes reading and navigating them effectively an essential skill for researchers.\nHowever, research papers can be difficult to navigate and understand, especially for early-stage researchers. Additionally, the ability to reproduce research findings is a key component of research, and this is where the ability to navigate research papers becomes crucial.\nBy developing the skills needed to read and navigate research papers, researchers can more effectively understand the existing state of research in their field, identify areas of research that require further investigation, and reproduce research findings.\nLearning Outcomes At the end of this workshop, participants will be able to:\nUnderstand the structure and components of a research paper Develop a framework for approaching reading papers Skim through papers to identify the most relevant parts Analyze the key results and methods in a paper Determine the reproducibility of the results presented in a paper Critically evaluate the methodology, assumptions, and limitations of a paper Evaluate feasibility of reproducing results from papers with and without access to code Identify gaps in current research and propose future directions for research. Through these learning outcomes, participants will gain the skills and knowledge necessary to become more efficient and effective readers of research papers, and to evaluate the reliability and relevance of the research presented. See the workshop structure page for more information about general workshop format.\n","permalink":"http://mindthemath.com/workshops/papers/","summary":"Motivations Research papers are an integral part of academic research. They are the primary means of disseminating research findings, which makes reading and navigating them effectively an essential skill for researchers.\nHowever, research papers can be difficult to navigate and understand, especially for early-stage researchers. Additionally, the ability to reproduce research findings is a key component of research, and this is where the ability to navigate research papers becomes crucial.","title":"How to Read and Navigate Research Papers"},{"content":"Note this is a draft of an upcoming workshop containing a super-set of contextual information about this complicated topic. This workshop is not yet complete but please express interest in its development by reaching out.\nMotivations Partial differential equations (PDEs) are essential tools for describing many natural phenomena, ranging from fluid dynamics to quantum mechanics. In recent years, there has been a growing interest in variational approaches to PDEs, which are based on the minimization of certain functionals defined on spaces of functions.\nThese techniques have many advantages, including a rigorous mathematical foundation, strong connections with optimization and machine learning, and the ability to tackle problems that are difficult or even impossible to solve with other methods.\nThe purpose of this workshop is to provide an introduction to the theory of variational PDEs, with a focus on the practical aspects of solving and analyzing these equations using Python.\nParticipants will learn how to formulate and solve several classic problems in mathematical physics, such as the heat, wave, and Laplace equations, using variational techniques.\nThey will also learn how to use modern software tools such as FEniCS and Jupyter notebooks to implement and visualize the solutions. By the end of the workshop, participants should have a good understanding of the basic concepts and techniques of variational PDEs, as well as the confidence to apply them to their own research problems.\nLearning Outcomes Sobolev Spaces: A Fundamental Concept in PDEs One of the key concepts in understanding and solving partial differential equations is the idea of Sobolev spaces. Sobolev spaces are a type of function space that are used to study the regularity of solutions to PDEs. They are named after the Russian mathematician Sergei Sobolev, who introduced them in the 1930s.\nIn this workshop, we will cover the basics of Sobolev spaces, including how they are defined, the different types of Sobolev spaces, and how they are used in the analysis of PDEs.\nWe will also go over examples of PDEs and discuss how Sobolev spaces can be used to determine whether or not a solution exists and how smooth that solution is. By the end of the workshop, participants will have a solid understanding of Sobolev spaces and how they can be applied to the study of PDEs.\nBilinear Forms Bilinear forms are important in functional analysis, particularly in the study of partial differential equations. The reason for this is that they provide a way of defining function spaces such as Sobolev spaces, which are central to the analysis of PDEs.\nBy defining a bilinear form on a space of functions, we can use it to define a norm on that space, which in turn allows us to measure the size of functions in the space.\nThis is particularly useful when studying PDEs, as it allows us to formulate weak solutions of the PDE in terms of the bilinear form. Sobolev spaces are a class of function spaces that are particularly well-suited to this approach, and are defined in terms of a particular bilinear form known as the Sobolev inner product.\nBy using bilinear forms to define function spaces, we can apply tools from functional analysis to the study of PDEs, and use this to obtain information about the behavior of solutions to these equations.\nA bilinear form is a function $$B:H^1_0(\\Omega) \\times H^1_0(\\Omega) \\rightarrow \\mathbb{R}$$ that is linear in each of its arguments. For example, consider the following weak formulation of the heat equation:\n$$\\int_\\Omega u v_t + \\nabla u \\cdot \\nabla v - \\int_{\\partial \\Omega} \\alpha u v = \\int_\\Omega f v$$ where \\(u\\) and \\(v\\) are functions in the Sobolev space \\(H^1_0(\\Omega)\\) and \\(f\\) is a function. The left-hand side of this equation defines a bilinear form \\(B(u,v)\\). The first term in the integral is a linear form in \\(v\\), the second term is a bilinear form in \\(u\\) and \\(v\\), and the third term is a bilinear form in \\(u\\) and \\(v\\). The variational formulation of the heat equation can be derived by multiplying the PDE with a test function \\(v\\) and integrating over the domain \\(\\Omega\\): $$ \\begin{aligned} \\frac{\\partial u}{\\partial t} - \\nabla^2 u \u0026= f \u0026\\text{ in } (0,T)\\times \\Omega,\\\\ u \u0026= 0 \u0026\\text{ on } (0,T)\\times \\partial \\Omega,\\\\ u \u0026= g \u0026\\text{ at } t=0 \\text{ on } \\Omega. \\end{aligned} $$ $$ \\int_{\\Omega} \\frac{\\partial u}{\\partial t} v dx - \\int_{\\Omega} \\nabla^2 u v dx = \\int_{\\Omega} f v dx, $$ Using integration by parts, the second term can be written as $$ -\\int_{\\Omega} \\nabla^2 u v dx = \\int_{\\Omega} \\nabla u \\cdot \\nabla v dx - \\int_{\\partial \\Omega} v \\frac{\\partial u}{\\partial n} ds $$ where \\(\\frac{\\partial u}{\\partial n}\\) is the derivative of \\(u\\) normal to the boundary of the domain. Substituting this expression back in to the original equation and assuming that the boundary term vanishes due to the boundary conditions gives: $$ \\int_{\\Omega} \\frac{\\partial u}{\\partial t} v dx + \\int_{\\Omega} \\nabla u \\cdot \\nabla v dx = \\int_{\\Omega} f v dx $$ The first term can be integrated by parts in time to yield: $$ \\int_{\\Omega} \\frac{\\partial u}{\\partial t} v dx = \\int_{\\Omega} u \\frac{\\partial v}{\\partial t} dx - \\int_{\\partial \\Omega} v \\frac{\\partial u}{\\partial n} ds. $$ Substituting this expression back into the equation, we obtain the weak form of the heat equation, given by $$ \\int_{\\Omega} u \\frac{\\partial v}{\\partial t} dx + \\int_{\\Omega} \\nabla u \\cdot \\nabla v dx = \\int_{\\Omega} f v dx. $$ The bilinear form of the variational problem is given by: $$ a(u,v) = \\int_{\\Omega} \\nabla u \\cdot \\nabla v dx $$ where \\(u,v\\) are functions in some suitable function space. FEniCS In this workshop, we will be using FEniCS, a popular open-source finite element software package, to solve various PDEs. FEniCS offers a modern C++/Python interface, automated code generation, and a range of finite element function spaces, providing a powerful and easy-to-use platform for implementing and solving PDE problems.\nWe will introduce the basic syntax and structure of FEniCS, along with its key features such as mesh generation, function space specification, and variational problem formulation. We will also demonstrate how FEniCS can be used to solve PDEs of various types, including elliptic, parabolic, and hyperbolic equations. By the end of the workshop, participants will have gained a strong foundational knowledge of FEniCS, as well as the skills and tools to implement and solve PDE problems using this software.\nFEM Finite element method (FEM) is a numerical technique used to approximate solutions to differential equations. It works by dividing a complex problem into smaller, simpler parts called finite elements. Then, the differential equations are converted into algebraic equations that can be solved to find the approximate solution for each element.These solutions are then combined to obtain an overall solution for the entire problem.\nThe finite element method is often used for solving partial differential equations because it allows for the approximation of solutions in a more efficient and accurate manner compared to traditional analytical methods. FEM is widely used in engineering, physics, and applied mathematics.\nFDM Finite difference methods are one approach to solve partial differential equations (PDEs), including variational PDEs. They approximate derivatives by finite differences and convert the differential equations into algebraic equations, which can be solved numerically using a computer.\nFinite difference methods are a popular numerical method for solving partial differential equations, particularly those that are defined over a discrete grid of points. The method relies on approximating the derivatives of the function at each point on the grid using finite difference quotients. By using this approximation, the original partial differential equation can be transformed into a system of linear equations that can be solved numerically.\nFinite difference methods have a long history in numerical analysis and are particularly useful for solving problems in areas such as physics, engineering, and finance. However, the accuracy of the method depends heavily on the choice of discretization scheme and the size of the grid used, and careful consideration is required to ensure that the method is both accurate and computationally efficient.\n","permalink":"http://mindthemath.com/blog/pdes/","summary":"Information about an upcoming workshop on variational partial differential equations.","title":"Variational PDEs"},{"content":"Nginx Setup Some useful nginx config walk-through that helped me in the setup of a staging server to test out this website.\n","permalink":"http://mindthemath.com/blog/staging/","summary":"Helpful resources for nginx configuration","title":"Nginx Resources"},{"content":"Motivations Docker images are a popular way of packaging and distributing software. With the rise of different computing platforms and architectures, it has become increasingly important to be architecture-aware when creating and distributing images. One way to achieve this is by building images that are compatible with multiple platforms, which can be accomplished using multi-arch images.\nComputer Architecture In order to build architecture-aware Docker images that can run on different types of machines, it is important to have a basic understanding of computer architecture. Specifically, it is helpful to know about the instruction set architecture (ISA) used by the different processors that you are targeting, as well as any operating system requirements.\nInstruction Sets Instruction sets are the collection of instructions that a processor is capable of executing. Each instruction in the set is encoded as a unique sequence of bits and is capable of performing a specific operation, such as addition, multiplication, or comparison. Different processors have different instruction sets, and each instruction set has its own set of operations that can be performed. Understanding instruction sets is important for computer architecture, as it enables one to understand the capabilities and limitations of a processor and how it can be used to perform different types of computations.\nFor example, if you are building images that will run on both x86-64 and ARM processors, you will need to be aware of the differences in their respective ISAs and any specific optimizations or limitations of each. Additionally, you may need to take into account the endianness of the processors, as this can affect how data is stored and retrieved.\nIoT On a practical note: multi-architecture Docker images are very helpful for IoT devices. Since IoT devices come in different architectures, having multi-architecture images will allow the same application to run on all the different architectures. It reduces the burden of having to create different images for each architecture and also speeds up the development process, allowing developers to focus on other aspects of their application.\nBy using multi-architecture Docker images, IoT devices will be able to run the same application, regardless of the architecture of the device, reducing the maintenance burden of having to create separate applications for each device.\nWhat are Multi-Arch Images? A multi-arch (multiple architecture) image contains multiple platform-specific layers, each of which is optimized for a specific hardware architecture. By using the same tag for each platform-specific layer, you can ensure that users will always pull the correct layer for their architecture.\nThis approach is especially useful when working with teams of developers and users who are using different hardware architectures, as it simplifies the deployment process and ensures that everyone is running the same version of the software.\nBuilding Multi-Arch Images To start building multi-arch Docker images, it\u0026rsquo;s necessary to have an understanding of how CPU architectures work and how the Docker platform supports this functionality. Essentially, Docker allows for the building of a single image that can run on multiple platforms, which enables cross-platform development and deployment.\nTo accomplish this, one needs to use the manifest command to create and manage manifests, which allow for the specification of different images for different platforms. An example of this would be specifying linux/amd64 and linux/arm/v7 images for an application to support different types of devices.\ndocker buildx Here\u0026rsquo;s an example of how to tag and build a manifest for a Docker image:\ndocker buildx build --platform linux/amd64,linux/arm64 -t myimage:latest . docker manifest create myimage:latest myimage:latest-amd64 myimage:latest-arm64 docker manifest push myimage:latest This example uses the buildx command to build an image for two platforms (linux/amd64 and linux/arm64) and tag it as myimage:latest. The manifest create command then creates a manifest list that maps the myimage:latest tag to the two platform-specific images that were just built. Finally, the manifest push command pushes the manifest list to Docker Hub (or another container registry).\nCombining Images from Different Machines If you\u0026rsquo;re not using buildx, you can still build images for different architectures by using separate Dockerfiles for each architecture, and then building and tagging each image separately.\nFor example, you might have two separate Dockerfiles - one for x86 architecture and one for ARM architecture - and you would build and tag each one separately using the appropriate tag for each architecture. Once you\u0026rsquo;ve built the images for each architecture, you can then use the docker manifest command to create a multi-arch manifest for the images, which can then be pushed to a registry and used to automatically pull the correct image for the architecture on which it\u0026rsquo;s being run.\nHere\u0026rsquo;s an example of how to build multi-arch Docker images on separate machines:\nOn machine 1 (x86 architecture), build and tag the image for the x86 architecture:\ndocker build -t myimage:latest-x86 . On machine 2 (ARM architecture), build and tag the image for the ARM architecture:\ndocker build -t myimage:latest-arm64 . Push the images to a container registry from their respective machines:\ndocker push myimage:latest-x86 docker push myimage:latest-arm64 From one of the two machines, pull the other image, then combine them like this into a manifest:\ndocker manifest create myimage:latest myimage:latest-x86 myimage:latest-arm64 Annotate the manifest to indicate the supported architectures:\ndocker manifest annotate myimage:latest myimage:latest-arm64 --os linux --arch arm64 --variant v8 docker manifest annotate myimage:latest myimage:latest-x86 --os linux --arch amd64 Push the manifest to the container registry:\ndocker manifest push myimage:latest ","permalink":"http://mindthemath.com/blog/arch-aware-docker/","summary":"Building Docker Images that Can Run on Any Architecture: A Guide for Developers","title":"Architecture-Aware Docker"},{"content":"tl;dr Turn the SettingWithCopyWarning into an explicit error that you are forced to fix by setting\nimport pandas as pd pd.set_option(\u0026#34;mode.chained_assignment\u0026#34;, \u0026#34;raise\u0026#34;) Problem pandas is complaining to you with the following warning (which only shows up on first execution):\nSettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Solution Use pd.set_option(\u0026quot;mode.chained_assignment\u0026quot;, \u0026quot;raise\u0026quot;) to force yourself to actually fix the underlying piece of offending code.\nMost often you will need to explicitly call .copy() on a sub-dataframe you want to manipulate, particularly when you want to ensure that changes you make to the \u0026ldquo;slice\u0026rdquo; of the DataFrame do not propagate back to the original object. Credit to this article for explaining the problem and solution.\n","permalink":"http://mindthemath.com/blog/pd-set-copy/","summary":"How to fix an annoying warning.","title":"About the Pandas SettingWithCopyWarning"}]